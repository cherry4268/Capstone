#### WEEKLY UPDATE 9 -
I realized some of my forecasts looked off the further away they were so I spent most of my time re-evaluating what went wrong. I've so far pieced together the issue isn't
from my model but rather the coding logic. The issue is more so with my Panel ARDL model rather than my XGBoost model. In addition to this I've worked on my UI and have a basic
set up but want to add more detail to it before it's finished. I've also started writing out the report.




### WEEKLY UPDATE 8 - 
I've combined the ARDL panel model and XGBoost models as they had the best results in terms of accuracy and highest R-squared values. I have more in-depth documentation
on why I chose these models along with notes on their short comings or areas where I had to tweak the variables. My next steps are to focus on the UI aspect. I haven't
done much since building the initial map and since then some of the variables have changed. I also plan to do more work on my report. 


## WEEKLY UPDATE 7 -
I've now tried out fixed effects panel model, ARDL panel model, XGBoost, Random Forest, and a basic linear regression. I've also included tests for each model's assumptions and
adjusted my variables where it was necessary. I am now working on combining my top two, most likely the Fixed Effects as a first step and the XGBoost or Random Forest as the
second step. I will now be focusing on incorporating this into my front end and developing a proper UI.


## WEEKLY UPDATE 6 - 
I've tried out a couple of the models with varying success; fixed effects panel model, ARDL panel model, and XGBoost. I've also added real Price Consumption Expenditure data
from the BEA as my dependent variable and sort of cost of living index. I plan on trying out the rest of the models and then deciding which one works best.

## WEEKLY UPDATE 5 - I am now working on model selection. Here are some current ideas:
- fixed effects panel mode; handles time-invariant heterogeneity, not great for complex non-linearities | ASSUMES: exogeneity, homoskedasticity
- random effects panel model; more efficient than fixed effects but stricter assumptions | ASSUMES: random effects are uncorrelated with explanatory variables
- panel vector autoregression; captures dynamic relations between multiple time series but LOTS to calculate, many parameters | ASSUMES: stationarity, homogenous dynamics across similar states
- ADL: captures dynamic relations, can handle structural breaks but lots of parameters and potential overfitting | ASSUMES: no autocorrelation, correct lag specification, stationarity
- GBM/XG Boost; excellent with temporal data, handles non-linearities and interactions, robust to outliers but needs hyperparameter-tuning | ASSUMES: sufficient data
- Random Forest with Time Features; same as above but less prone to overfitting BUT can't extrapolate beyond training range much

I'm currently leaning towards a 2-stage hybrid model:
- stage 1; Panel ADL with fixed effects - captures state-specific differences, dynamic relationships, lagged effects, and structural breaks
- stage 2; GBM/XG Boost - model residuals from the first stage, captures more complex relationships

## WEEKLY UPDATE 4 - I forgot to do this earlier, but I've made significant changes to the data criteria after extensive searching and limited results. 
- Due to the lack of robust data for the last few years, I am now using data from 2015-2022 to predict future prices. I chose this time range because it includes time
  pre-COVID to ensure I'm not using exaggerated prices from those years. This will also help me to check how accurate my predictive model is as I can check it against 
  the data that does exist for 2023 and 2024.
- My data now covers housing, food, transportation, and utilities. The food dataset comes from an expert in regional price data I contacted. She uses the same source (map the meal)
  to cover the average cost per meal by state annually. The transportation data is covered by average auto-insurance rates I found but will also include gas prices. Utilities will 
  cover natural gas, electricity, water, telecomm communications, and garbage/waste removal. 
I've done a bit of EDA within R studio to map out changes in prices over time. The data all look relatively normal and now that I'm working with annual data there is less of a concern for
seasonality and there doesn't seem to be other significant trends that would need further normalization.


## WEEKLY UPDATE 3 - This week I talked with an expert on regional price data about my datasets and she recommended an entirely new food dataset from a website called
map the meal. This website tracks data on food insecure individuals, but also provides an annual report of what a single meal would cost per country. This data is already
cleaned and seasonally adjusted, so I wouldn't have to worry about adjusting the data like I was last week. She also helped talk me through what aggregating the data would 
look like for my other datasets since they are recorded by month. I'm also adding utilities as one of my predictors, also from the Bureau of Labor Statistics.
me through what aggregating the data should look like.



## WEEKLY UPDATE 2 - I had uploaded this content much earlier on Monday to Blackboard but added it here later
This week I collected more data and began to normalize the datasets. The new datasets are from the Bureau of Labor Statistics CPI data. 
In terms of normalization, I'm experimenting with STL Decomposition which will give me a breakdown of seasonal, trend, and remainder (residual) outliers. 
I'm also utilizing X-13ARIMA-SEATS, which is a package used by the US Census Bureau specifically for seasonal adjustment including holidays, weather phenomena, etc. 
The normalization I'm doing in R-studio since the package for X-13ARIMA-SEATS is included with R-studio and is difficult to install otherwise. 
I plan to replace the data currently sitting in my github repo with the seasonally adjusted data once I'm done. 
Some datasets from the grocery folder such as as "cereal_bakery" or "fish_seafood" need to be removed but all datasets I'm currently planning on using are up-to-date in the READ.me
